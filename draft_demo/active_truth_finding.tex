% Active Ensembling for Truth Finding
\section{Ensembling for Truth Finding}\label{ensembling}
We present in this section an adaptive truth finding approach which uses active ensembling in order to 
adaptively learn about an optimal set of truth finding algorithms that outperforms any individual
technique on any given dataset. Our learning approach will actively involve users for the correct
labels (or answers) of a sample of queries that cause maximal disagreement amongst our classifiers.

\subsection{Ensemble-Based Active Learning}
\begin{itemize}
\item donner idée générale pour introduire  ce qu'est l'ensembling
\item on a besoin de le faire dans le contexte de truth discovery car aucune methode ne bat toutes les autres dans tous les cas de figure
\item donc on combine les methodes : il y plusieurs façon de combiner par ex. consensus de méthodes, etc.
\item expliquer quelles méthodes on combine avec leurs avantages et inconvénients
\end{itemize}

\medskip
An ensemble-based active learning, or commonly ensembling, is a semi-supervised learning approach that tries
to figure out an optimal ensemble of classifiers for a given classification problem by actively querying an 
oracle, e.g., a human being, about the labels of a sample of data items. Ensembling, thereby, enables to perform
classification consistently well across datasets without having to determine a \emph{priori} a suitable classifier 
type.

In our context, the truth finding algorithms correspond to our set of classifiers. The underlying \emph{binary classification} 
problem consists of assigning the correct truth label to a set of claims about given user queries. Indeed, a truth finding 
algorithm is formally a mapping $\textsf{TF}:~\claimset \mapsto \{\TRUE, \FALSE\}$ which associates to each claim in $\claimset$
either $\TRUE$ or $\FALSE$. A good truth finding algorithm provides  predictions that are close to the actual world. Unfortunately, 
a well known property, e.g., as shown in~\cite{Li12, Wagui14}, of existing truth discovering algorithms remains their sentivity
to certain application domains or datasets. As a consequence, there is no actual approach that outperforms the others on all types 
of datasets. On the other hand, truth finding is hard in practical scenarios because there is often no prior knowledge guiding to
the selection, beforehand, of an optimal algorithm, in particular when the context is dynamic. More importantly, a large set of labeled 
examples (or ground truth) for evaluating the precisions of the algorithms is expensive to obtain in real applications. 

In general, human being has a certain background knowledge about some real-world facts. Such a knowledge can serve as a valuable and inexpensive source of labels for a 
rather reasonable number of data items. However, having this partial ground truth from users is not sufficient in order to definitively decide about an optimal truth
finding strategy because it can change over time as we obtain more information from sources, e.g. when claims are continuously extracted by TextRunner for answering
new incoming queries. Therefore, there is a need for an adaptive approach able to dynamically figure out the optimal truth finding strategy when users' feedbacks and 
new knowledge about the world are available. We believe that active ensembling should be helpful to this end.

We put forward and demonstrate an approach which combines truth discovery and open information extraction with ensemble-based active learning for adaptively learning about the optimal 
ensemble of truth finding algorithms when the OpenIE system is gradually querying and labeled examples from users are available.
As we shall show later, we will actively involve users to obtain the truth about a sample of particular facts during the learning process. The way this sampling is defined and
chosen is crucial for the effectiveness of the active learning. Several sample selection strategies, e.g., random sampling, query by committee, or support vector machine models,
have been proposed for the definition of the type of selected data items along the size of the sample; we defer to~\cite{burr12} for more details about active machine learning.
In this study, we use \emph{query by committee} (QBC) for ensemble-based active learning. QBC states that the best data items to select for labels are those that cause 
the \emph{maximal disagreement} among the predictions of an ensemble of diverse but partially accurate classifiers during active learning. Furthermore, we seek to provide
an adaptive active learning by looking for an optimal ensemble given a larger set of input classifiers.
\lamine{Peut \^etre qu'il y a mieux que QBC ?}

To learn about an optimal ensemble from a diverse set of classifiers, we have considered
twelve well established truth finding algorithms in the literature, having three different types 
according to their specificities. Note that diversity offers better result in active learning than
using homogenoeus classifiers (see~\cite{Lu15}). We briefly present each considered class of truth
discovering algorithms in the following.

\begin{enumerate}
 \item \textbf{Iterative techniques:} TruthFinder~\cite{YinHY08}, Cosine, 2-Estimates and 3-Estimates~\cite{GallandAMS10}, 
 AccuNoDep~\cite{DongBS09}
 \item \textbf{EM based techniques:} MLE~\cite{WangKLA12}, LTM~\cite{ZhaoRGH12}, SimpleLCA and GuessLCA~\cite{PasternackR13}
 \item \textbf{Dependency detection based techniques:} Depen, Accu, and AccuSim~\cite{DongBS09}
\end{enumerate}

\lamine{Peut \^etre qu'il existe une meilleure classification ?}


\subsection{Truth Finding with Active Ensembling}
\begin{itemize}
 \item notre approche que l'on défend ici dans la démo est  semi supervisée en impliquant de l'utilisateur de façon active
en lui demandant s'il peut confirmer des faits (facts)
\item si on a une ground truth partielle on la "rejoue" cas par cas
\end{itemize}

\medskip

We introduce an adaptive truth finding approach using active ensembling in order to discover an hybrid, i.e., an optimal
ensemble of truth finding algorithms, on which we could efficiently tells the truth for the output of OpenIE systems.
Our approach first searches for the optimal subset of truth finding algorithms using active ensembling and then it 
tells the truth by \emph{majority voting} on the predictions of the obtained ensemble. As we detail below, we follow the intuition
the adaptive strategy, proposed in~\cite{Lu15}, using QBC in order to perform active ensemble over our set of heterogeneous 
ensembles of truth finding algorithms. Such a strategy efficiently finds an optimal ensemble with the best ratio of classifier
types and the smallest size given a search space.
\lamine{Adaptative ou non-adaptative apprentissage active?}


Similarly to~\cite{Lu15}, we set up an iterative active ensembling process which considers the set of claims $\claimset{}$, a set of testing claims
$\claimset_{T}$, an adaptation set which is used for adapting the configuration of the ensemble, an initial ensemble size, a vector where each element
represents the initial number of instances of a type of truth finding algorithms, a window size and a stopping criteria. At each iteration, the algorithm
chooses the set of claims about the same query for asking labels to the user. Once the requested labels are obtained from the user, the algorithm puts the claims
along with their labels into the training set. \lamine{Une esquisse du process d'apprentissage active}

\begin{enumerate}
\item The algorithm starts with an \emph{initialization phase} where it sets initial values of some parameters and trains an initial ensemble over an initial 
training set.
 \item The algorithm then records the vote entropy of each query based on the truth told by each algorithm in the committee on the claims in $\claimset{}$
 \item It selects the set of claims associated the query with the maximum vote entropy and queries their truth labels from the user
 \item The set of acquired truth labels, together with the corresponding claims, are added into the training set
 \item At this stage the algorithm uses an \emph{adaptation algorithm} in order to update the parameter of the ensemble
 \item The newly generated ensemble is trained on the training set 
\end{enumerate}

The steps 2--6 of the active learning algorithm are repeated until the stop criteria is satisfied. 
At the end of the active learning process, the truth discovery is finally realized by performing 
a majority vote on the predictions of the final inferred ensemble. 
