% Active Ensembling for Truth Finding
\section{Ensembling for Truth Finding}\label{ensembling}
We present in this section an adaptive truth finding approach which uses active ensembling in order to 
adaptively learn about an optimal set of truth finding algorithms that outperforms any individual
technique on any given dataset. Our learning approach will actively involve users for the correct
labels (or answers) of a sample of queries that cause maximal disagreement amongst our classifiers.

\subsection{Ensemble-Based Active Learning}
\begin{itemize}
\item donner idée générale pour introduire  ce qu'est l'ensembling
\item on a besoin de le faire dans le contexte de truth discovery car aucune methode ne bat toutes les autres dans tous les cas de figure
\item donc on combine les methodes : il y plusieurs façon de combiner par ex. consensus de méthodes, etc.
\item expliquer quelles méthodes on combine avec leurs avantages et inconvénients
\end{itemize}

\medskip
An ensemble-based active learning, or commonly ensembling, is a semi-supervised learning approach that tries
to figure out an optimal ensemble of classifiers for a given classification problem by actively querying an 
oracle, e.g., a human being, about the labels of a sample of data items. Ensembling, thereby, enables to perform
classification consistently well across datasets without having to determine a \emph{priori} a suitable classifier 
type.

In our context, the truth finding algorithms correspond to our set of classifiers. The underlying \emph{binary classification} 
problem consists of assigning the correct truth label to a set of claims about given user queries. Indeed, a truth finding 
algorithm is formally a mapping $\textsf{TF}:~\claimset \mapsto \{\TRUE, \FALSE\}$ which associates to each claim in $\claimset$
either $\TRUE$ or $\FALSE$. A good truth finding algorithm provides  predictions that are close to the actual world. Unfortunately, 
a well known property, e.g., as shown in~\cite{Li12, Wagui14}, of existing truth discovering algorithms remains their sentivity
to certain application domains or datasets. As a consequence, there is no actual approach that outperforms the others on all types 
of datasets. On the other hand, truth finding is hard in practical scenarios because there is often no prior knowledge guiding to
the selection, beforehand, of an optimal algorithm, in particular when the context is dynamic. More importantly, a large set of labeled 
examples (or ground truth) for evaluating the precisions of the algorithms is expensive to obtain in real applications. 

In general, human being has a certain background knowledge about some real-world facts. Such a knowledge can serve as a valuable and inexpensive source of labels for a 
rather reasonable number of data items. However, having this partial ground truth from users is not sufficient in order to definitively decide about an optimal truth
finding strategy because it can change over time as we obtain more information from sources, e.g. when claims are continuously extracted by TextRunner for answering
new incoming queries. Therefore, there is a need for an adaptive approach able to dynamically figure out the optimal truth finding strategy when users' feedbacks and 
new knowledge about the world are available. We believe that active ensembling should be helpful to this end.

We put forward and demonstrate an approach which combines truth discovery and open information extraction with ensemble-based active learning for adaptively learning about the optimal 
ensemble of truth finding algorithms when the OpenIE system is gradually querying and labeled examples from users are available.
As we shall show later, we will actively involve users to obtain the truth about a sample of particular facts during the learning process. The way this sampling is defined and
chosen is crucial for the effectiveness of the active learning. Several sample selection strategies, e.g., random sampling, query by committee, or support vector machine models,
have been proposed for the definition of the type of selected data items along the size of the sample; we defer to~\cite{burr12} for more details about active machine learning.
In this study, we use \emph{query by committee} (QBC) for ensemble-based active learning. QBC states that the best data items to select for labels are those that cause 
the \emph{maximal disagreement} among the predictions of an ensemble of diverse but partially accurate classifiers during active learning. Furthermore, we seek to provide
an adaptive active learning by looking for an optimal ensemble given a larger set of input classifiers.
\lamine{Peut \^etre qu'il y a mieux que QBC ?}

To learn about an optimal ensemble from a diverse set of classifiers, we have considered
twelve well established truth finding algorithms in the literature, having three different types 
according to their specificities. Note that diversity offers better result in active learning than
using homogenoeus classifiers (see~\cite{Lu15}). We briefly present each considered class of truth
discovering algorithms in the following.

\begin{enumerate}
 \item \textbf{Iterative techniques:} TruthFinder~\cite{YinHY08}, Cosine, 2-Estimates and 3-Estimates~\cite{GallandAMS10}, 
 AccuNoDep~\cite{DongBS09}
 \item \textbf{EM based techniques:} MLE~\cite{WangKLA12}, LTM~\cite{ZhaoRGH12}, SimpleLCA and GuessLCA~\cite{PasternackR13}
 \item \textbf{Dependency detection based techniques:} Depen, Accu, and AccuSim~\cite{DongBS09}
\end{enumerate}

\lamine{Peut \^etre qu'il existe une meilleure classification ?}


\subsection{Truth Finding with Active Ensembling}
\begin{itemize}
 \item notre approche que l'on défend ici dans la démo est  semi supervisée en impliquant de l'utilisateur de façon active
en lui demandant s'il peut confirmer des faits (facts)
\item si on a une ground truth partielle on la "rejoue" cas par cas
\end{itemize}

\medskip

We propose an adaptive truth finding approach using active ensembling in order to discover an hybrid, i.e., an optimal
ensemble of truth finding algorithms, on which we could efficiently tells the truth for the output of OpenIE systems.
Our approach first searches for the optimal subset of truth finding algorithms using active ensembling and then it 
tells the truth by \emph{majority voting} on the predictions of the obtained ensemble.

Let denote by $\qset$ the set of successive user queries processed 
by the information extraction system. For each query $\query$ in $\qset$ about the fact $\fact{\query}$ we have the corresponding
sets of claims $\claimset_{\query}$ returned by the extractor. We refer to the entire set of all claims by $\claimset$ regarding $\qset$.
We assume that $\claimset$ contains labeled and unlabeled claims where labeled claims, corresponding to our partial ground truth, are those 
for which we  know whether they are correct or not by querying the user. In contrast, we do not know yet the truth about unlabeled claims and
would like to discover by using the best ensemble of truth finding algorithms. We refer respectively to labeled and unlabeled set of claims by 
$\claimset^{\mathsf{L}}$ and $\claimset^{\mathsf{U}}$. Given a base learning algorithm $\mathsf{X}$, a number $k$ of fixed act iterations, and 
a fixed size $m$ of a sampling, we perform truhth finding with ensembling on our set of truth finding algorithms as follows.

\begin{enumerate}
 \item We first train our ensemble of truth finding algorithms (representing here our set of classifiers)  on the current set of labeled claims
 $\claimset^{\mathsf{L}}$ \lamine{How the base learning algorithm should be implemented?}
 \item We then pass our set of unlabeled claims in $\claimset^{\mathsf{U}}$ to the ensemble of truth finding algorithms
 \item Each algorithm in the current ensemble predicts the label of each claim
 \item The claims that induce the most label prediction disagreement are queried for their labels and added to the training set
 \item We select a subset $\mathsf{T}$ of the $m$ claims that induce the most label prediction disagreement
 \item We request the labels of these $m$ claims to the user 
 \item We lastly remove claims in $\mathsf{T}$ from $\claimset$ and add them together with their acquired truth labels to $\claimset^{\mathsf{L}}$.
\end{enumerate}

We repeat the process above $k$ times or until the ensemble meets some pre-defined criteria, e.g. when the size of the ensemble
reaches a certain pre-defined value. At the end of the active learning process, the prediction, i.e., truth discovery, is made 
by taking the majority vote of the resulting ensemble members. 
