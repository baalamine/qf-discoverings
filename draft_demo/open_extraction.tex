% Information Extraction Technique
\selectlanguage{english}
\section{Open Information Extraction}
\begin{itemize}
 \item décrire le type d'information auquel on s'intéresse par exemple "factoid claim"
 \item decrire le systeme sur lequel on se base
 \item décrire comment on transforme l'output de OpenIE
 \item donner qq exemples
\end{itemize}

\medskip

We are seeking to demonstrate in this paper the usefulness of truth discovery on large sets of 
``factoid" claims about real-world facts which are obtained when querying open information extraction
systems. These claims are usually extracted by information extractors from unreliable and conflicting 
Web sources.
A ``factoid" claim, e.g.,  \textsf{Barack Obama was born in Kenya}, often refers to piece of 
information that is accepted as true, without any prior verification, because of its frequent redundancy
over multiple different sources. Multiple distinct Web claims of such a kind about a given real life fact 
are simultaneously available in practice. We focus, therefore, on conflicting claims returned by a common
open Web information extraction system, here TextRunner, as possible answers to users' queries. Concretly, given user input, we retrieve the set of candidates claims, together with the associated
sources, returned by TextRunner~\footnote{TextRunner is accessible online at \href{http://openie.allenai.o, rg/}{http://openie.allenai.org/}}
from a Web corpus. We then format this output in such a way that fits our truth discovering process.


\begin{figure*}[!ht]
\begin{subfigure}[TextRunner Extraction]{Un exemple d'extraction avec TextRunner}
\end{subfigure}
\hfill
\begin{subfigure}[Extracted Triplets]{Un exemple de tripl\'es extraites}
\end{subfigure}
\label{open}\caption{Data collection and formatting}
\end{figure*}

% OIE Input and Ouput
\paragraph*{Data collection} We analyze claims about real-world facts
from TextRunner.
TextRunner is an open Web information extraction system relying
on an unsupervised extraction process on a fixed collection of 
Web corpus consisting of Google, ClueWeb, News, Nell, and Wikipedia 
corpus. Each particular corpus aggregates information from multiple 
other Web sources which can be of different nature, e.g. domain specific
sources. When a query is issued, TextRunner first searches for the relevant
corpus to query, and it then extracts the possible answers from them. 
To do so, it performs, for efficiency concerns, a single data-driven
pass on the corpus to obtain the list of candidate relational tuples 
which might satisfy the user input query about a given real-world fact. 
In TextRunner, a user input query is typically a set of keywords
consisting  of two real-world \emph{entities} and a \emph{relation}. Such
a user input query $\query$ can be defined formally as a triplet $(\e{1}, \rel{}, \e{2})$
where $\e{1}$ and $\e{2}$ are real-world entities and $\rel{}$ is a relation. The argument $\rel{}$ 
specifies a possible relationship that might exist between the two given entities. None of the three 
arguments is mandatory, meaning that some of them can be not specified by the user when issuing 
his query. This captures the fact the user has a partial knowledge about the real world, which is 
common in practice. Let denote the fact the user input query $\query$ is referred to by $\fact{\query}$. 
TextRunner will find and extract a collection of Web claims related to this fact.

TextRunner's output is thus a set of candidate claims about a specific real-world fact.
This output is ranked by the system according to the number of sources that supports each
claim. A claim in this context can correspond to a tuple, a relation, or an real-world entity 
with respect to the given keywords. TextRunner also offers the ability, through Web hyperlinks, 
to access to the meta-data, e.g., the source, the type, the full corpus, etc., associated to each 
returned claim for further exploration. 

\paragraph*{Data formatting}
Given a user query $\query$ about a fact $\fact{\query}$, we consider the set $\claimset_{\query}$ 
of $n$ claims $\claim{1}, \ldots, \claim{n}$ extracted by TextRunner as answers to the query $\query$.
For each claim $\claim{i}$, $1\leq i\leq n$, we go through the attached Web hyperlink and extract the set
$\sset{\claim{i}}$ of sources which support it. We achieve such a extraction by using hand-written mapping 
rules. We have also assumed that the system returns only one claim per source for a given query, i.e., $\sset{\claim{i}}
\cap \sset{\claim{j}}=\emptyset$ for all $i\neq j$ with $1\leq i\leq j\leq n$. We have now, for the query
$\query$ about the fact $\fact{\query}$, the set of claims together with the sources. In order to fit to 
the input of the truth finding process, we need to format the data collected from TextRunner about the 
facts in a certain manner. As a consequence, for the fact $\fact{\query}$, we therefore consider every extracted 
claim $\claim{i}$ and generate a triplet $(\fact{\query}, \claim{i}, \source{j})$ for each source $\source{j}\in \sset{\claim{i}}$.
We finally obtain a collection of triplets $\{(\fact{\query}, \claim{}, \source{})\mid \claim{\query}\in \claimset, \source{} \in \sset{\claim{}}\}$ 
as the final formatting of the output of TextRunner regarding a given user input query $\query$ about a fact $\fact{\query}$. 

\lamine{Quelle types de requ\^etes souhaitons nous supporter?}

\lamine{Si on consid\'ere chaque requ\^ete s\'epar\'ee, on se retrouve \'a traiter un seul claim \'a chaque fois. G\'en\'eraliser \'a $k$ requ\^etes utilisateurs?}

\lamine{Peut \^etre qu'il serait interessant d'avoir une id\'ee de la distribution du nombre de conflits per query sur TextRunner }