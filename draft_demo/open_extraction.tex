% Information Extraction Technique
\selectlanguage{english}
\section{Open Information Extraction}
\begin{itemize}
 \item décrire le type d'information auquel on s'intéresse par exemple "factoid claim"
 \item decrire le systeme sur lequel on se base
 \item décrire comment on transforme l'output de OpenIE
 \item donner qq exemples
\end{itemize}

\medskip

We are seeking to demonstrate in this paper the usefulness of truth discovery on large sets of 
\emph{``factoid" claims} about real-world facts which are obtained when querying \emph{open information
extraction} (OpenIE) systems. These claims are usually extracted by information extractors from unreliable
and conflicting Web sources.
A ``factoid" claim, e.g.,  \textsf{Barack Obama was born in Kenya}, often refers to a piece of 
information that is accepted as true, without any prior verification, because of its frequent redundancy
over numerous sources. Multiple conflicting Web claims of such a type related to a specific real life fact 
are simultaneously available in practice. We draw, therefore, our attention on those conflicting claims 
returned by a common open Web information extraction system, here \emph{TextRunner}, as possible answers to users' queries. 
Concretely speaking, given a user input query (or key phrase), our main goal is to consider and improve the result of \emph{TextRunner}~\footnote{TextRunner is accessible online at \href{http://openie.allenai.o, rg/}{http://openie.allenai.org/}}
by running truth discovery in order to provide to the user the most reliable information.
We next briefly present \emph{TextRunner} system. Then, we detail how we format its output in such a that
it fits the input of our truth discovering process.


\begin{figure*}[!ht]
\begin{subfigure}[TextRunner Extraction]{Un exemple d'extraction avec TextRunner}
\end{subfigure}
\hfill
\begin{subfigure}[Extracted Triplets]{Un exemple de tripl\'es extraites}
\end{subfigure}
\label{open}\caption{Data collection and formatting}
\end{figure*}

% OIE Input and Ouput
\paragraph*{Information extraction} We consider and analyze 
conflicting claims about the same real-world facts from \emph{TextRunner}.
\emph{TextRunner}~\cite{Yates07, Etzioni08} is an OpenIE system relying
on an unsupervised Web extraction process over a predefined 
set of Web corpus consisting of \emph{Google}, \emph{ClueWeb}, \emph{News}, \emph{Nell}, and 
\emph{Wikipedia} corpus. Each particular corpus aggregates data from multiple 
other Web sources which are of various nature, for instance domain-specific Websites.
Queries are sent to \emph{TextRunner} by users through a form where 
they also can optionally specify their trusted corpus on the predefined set.
When a user query arrives, \emph{TextRunner} first finds the relevant
sources in the set of corpus, and then it extracts each possible answer from them
using natural language processing techniques and ontologies. 
To do so, it performs, for efficiency concerns, a single data-driven
pass on the corpus to obtain the list of candidate relational tuples 
which might satisfy the arguments of the user input query. 
A typical user query in \emph{TextRunner} is often about 
two real-world \emph{entities} and a certain \emph{relation} between them.
As a consequence, such a user query $\query$ can be defined formally as a triplet $(\e{1}, \rel{}, \e{2})$
where $\e{1}$ and $\e{2}$ are real-world entities and $\rel{}$ is a relation. The argument $\rel{}$ 
specifies a possible relationship between the two given entities $\e{1}$ and $\e{2}$. In general,
at least one among the three arguments is unkown, which captures a partial knowledge about the real world. 
In this context, \emph{TextRunner} tries to find out the actual possible values of unkown arguments given 
known ones by querying the Web.
%Let denote the fact the user input query $\query$ is referred to by $\fact{\query}$. 
%TextRunner will find and extract a collection of Web claims related to this fact.

The answer of \emph{TextRunner}, given a user query, is indeed a set of candidate claims 
which is ranked according to the number of sources supporting each possible claim. An answer, representing a claim made by a source, can correspond
a tuple, a relation, or a real-world entity.
%The system actually ranks this set of answers according to the
%number of sources  supporting each claim. 
A claim in this context can correspond to  TextRunner also offers the ability, through Web hyperlinks, 
to access to the meta-data, e.g., the source, the type, the full corpus, etc., associated to each 
returned claim for further exploration. 

\paragraph*{Information processing}
Given a user query $\query$ about a fact $\fact{\query}$, we consider the set $\claimset_{\query}$ 
of $n$ claims $\claim{1}, \ldots, \claim{n}$ extracted by TextRunner as answers to the query $\query$.
For each claim $\claim{i}$, $1\leq i\leq n$, we go through the attached Web hyperlink and extract the set
$\sset{\claim{i}}$ of sources which support it. We achieve such a extraction by using hand-written mapping 
rules. We have also assumed that the system returns only one claim per source for a given query, i.e., $\sset{\claim{i}}
\cap \sset{\claim{j}}=\emptyset$ for all $i\neq j$ with $1\leq i\leq j\leq n$. We have now, for the query
$\query$ about the fact $\fact{\query}$, the set of claims together with the sources. In order to fit to 
the input of the truth finding process, we need to format the data collected from TextRunner about the 
facts in a certain manner. As a consequence, for the fact $\fact{\query}$, we therefore consider every extracted 
claim $\claim{i}$ and generate a triplet $(\fact{\query}, \claim{i}, \source{j})$ for each source $\source{j}\in \sset{\claim{i}}$.
We finally obtain a collection of triplets $\{(\fact{\query}, \claim{}, \source{})\mid \claim{\query}\in \claimset, \source{} \in \sset{\claim{}}\}$ 
as the final formatting of the output of TextRunner regarding a given user input query $\query$ about a fact $\fact{\query}$. 

\lamine{Quelle types de requ\^etes souhaitons nous supporter?}

\lamine{Si on consid\'ere chaque requ\^ete s\'epar\'ee, on se retrouve \'a traiter un seul claim \'a chaque fois. G\'en\'eraliser \'a $k$ requ\^etes utilisateurs?}

\lamine{Peut \^etre qu'il serait interessant d'avoir une id\'ee de la distribution du nombre de conflits per query sur TextRunner }