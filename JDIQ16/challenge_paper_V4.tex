% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
%\usepackage{mathptm}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[ruled]{algorithm2e}
\usepackage{csquotes}
\usepackage{paralist}
\usepackage{needspace}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}
% Metadata Information
\acmVolume{6}
\acmNumber{6}
\acmArticle{6}
\acmYear{2015}
\acmMonth{11}

% Space crunch
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
                                       {2ex \@plus1ex \@minus .2ex}%
                                       {-1em}%
                                      {\sffamily\normalsize\bfseries}}        
\def\@biblabel#1{[#1]} % restore basic form of \@biblabel macro
\def\thebibliography#1{%
    \footnotesize
    \refsection*{{\refname}
        \@mkboth{\uppercase{\refname}}{\uppercase{\refname}}%
    }
    \list{\@biblabel{\@arabic\c@enumiv}}% %the default form of first arg is {}
       {\settowidth\labelwidth{\@biblabel{#1}}%
        \leftmargin\labelwidth
        \advance\leftmargin\bibindent
        \itemindent-\bibindent
        \itemsep2pt
        \parsep \z@
        \usecounter{enumiv}% % default is to use enumi
        \let\p@enumiv\@empty
        \renewcommand\theenumiv{\@arabic\c@enumiv}%
    }%
    \let\newblock\@empty
    \sloppy
    \sfcode`\.=1000\relax
}                                    
\makeatother


% Document starts
\begin{document}

% HEADERS
\markboth{L. Berti-Equille, M. L. BA}{Veracity of Big Data: Challenges of Cross-modal Truth Discovery}
% TITLE
\title{Veracity of Big Data: Challenges of Cross-modal Truth Discovery}
% AUTHORS AND AFFILIATION
\author{LAURE BERTI-EQUILLE, MOUHAMADOU LAMINE BA
\affil{Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar}}


\category{H.3.3}{Information Storage and Retrieval}{Information Search and Retrieval}
\category{H.2.0}{Database Management}{General}
\terms{Algorithms, Management, Verification}
\keywords{Truth Discovery, Fact-checking, Data Quality, Data Fusion, Information Extraction}
\acmformat{L. Berti-Equille, M. Lamine Ba 2015. Veracity of Big Data: Challenges of Cross-modal Truth Discovery.}


\begin{bottomstuff}
Author's address: L. Berti-Equille, M. L. Ba, H. M. Hammady, Qatar Computing Research Institute(Current address), Hamad
Bin Khalifa University, Tornado Tower 18th, P.O. Box 5825, Doha, Qatar.
\end{bottomstuff}

\maketitle


\section*{From Data Management to Truth Discovery Systems}
As online user-generated content grows exponentially,
the reliance on Web and social 
media data is increasing.  Truth discovery from 
the Web has significant practical importance as online rumor and misinformation
can have tremendous impacts on our society and everyday life.
One of the fundamental difficulties is that data can be biased, noisy, outdated, incorrect, misleading and
thus unreliable. Conflicting data from multiple sources amplifies this problem and veracity of data has to 
be estimated.

Beyond the emerging field of computational journalism and the success of online fact-checkers (e.g., FactCheck\footnote{\url{www.factcheck.org/}}, ClaimBuster\footnote{\url{idir.uta.edu/claimbuster}}), 
 truth discovery is a long-standing and challenging problem studied by many research communities in artificial intelligence, databases, and complex systems, and under various 
names: fact-checking, data and knowledge fusion,  information trustworthiness, credibility or information corroboration (see \cite{Berti2015} for a survey).  The ultimate goal  is to predict the truth label of a set of assertions claimed by multiple information sources
 and to infer sources' reliability with no or few prior knowledge. One 
major line of previous work aimed at iteratively computing and updating the source's
trustworthiness as a belief function in its claims, and then the belief score of each 
claim as a function of its sources' trustworthiness~\cite{YinHY08}. More complex probabilistic
models have then incorporated various aspects beyond source trustworthiness and claim belief such
as the dependence between sources~\cite{Dong:2010,DongBHS10a}, the correlation of claims~\cite{Pochampally2014},
the notion of evolving truth~\cite{DongBS09a}.  Recent contributions have further relaxed prior modeling assumptions
to deal with truth existence~\cite{ZhiZTGYJH15}, approximate truth discovery~\cite{Wang2015a,LiLGSZDFH14},  truth evolution~\cite{LiLGSZFH15,JiaWLG13},
and applications in the context of social media and crowd-sourcing~\cite{GaoLZFH15,MaLLQGZSZJH15}

 However, some studies showed that most of prior work suffers from scalability issues, complex parameter setting and non repeatability 
 of the results due to randomized initialization (see~\cite{BW2014} for a thorough analysis). Moreover, it is unlikely that one method dominates
 all others across all application domains and most approaches, relying on majority voting, fall short for pessimistic scenarios where most of the 
 sources are malicious and falsify information. As a consequence, it is currently unclear which techniques are the best suited as they are highly 
 data-dependent and quality performance evaluation depends on the available samples of ground truth data. 

 In this challenge paper, we argue that the next generation of data management systems need to 
 manage not only volume and variety of Big Data but most importantly veracity of data. Designing
 end-to-end truth discovery systems requires a fundamental paradigm shift largely driven by Machine Learning advances.
 It goes beyond adding new layers of data fusion heuristics or developing yet another probabilistic graphical truth discovery model. 
 Actionable, cross-modal, and Web-scale truth discovery is needed in such a perspective. It requires a transdisciplinary approach to
 analyze the dynamic and cross-modal dimensions of rapidly evolving networks of sources and multi-media contents. 
%\vspace{-0.1cm}
% ****************** ARCHITECTURE ****************************************
%\section{Challenges of Actionable Web-scale Truth Discovery}
%In this paper, we would like to highlight some of the challenges we consider the most promising in terms of research perspectives.
This paper highlights some of the challenges we deem as the most promising research perspectives towards an actionable, cross-modal, 
and Web-scale truth discovery.
%
\vspace*{-0.23cm}
\paragraph*{Cross-modal and Cross-lingual Truth Discovery} 
The agility of a truth discovery system is of utmost importance to efficiently extract and map information: 
\begin{inparaenum}[(i)]
\item from various languages;
\item in various data formats, structures, and semantics (e.g., texts, Web tables, structured data, etc.); and
\item  conveyed by various media and technologies (e.g., tweets, Instagram images, Youtube videos, Web pages, RSS feeds, etc.).
\end{inparaenum}
The main challenge is to address cross-modality and cross-language issues in the context of truth discovery where the linkage of various 
evidences in audio, image, video, text formats and languages has to be achieved as accurately as possible to corroborate events. This refers
to cross-media entity and event linking where Deep Learning has just started to bring some interesting results and can be leveraged for
cross-modal truth discovery.
\vspace*{-0.23cm}

\paragraph*{Timely and Actionable Truth Discovery} 
In a humanitarian context for example, truth discovery from quasi real-time data could save lives. To be actionable, information extraction and 
truth discovery computation need to be streamlined, prioritized depending on the level of emergency and incompleteness of available information,
and finally adjusted to the communities that will 
use the data (e.g., rescue team, NGOs). Time-dependent estimation and correction of observer bias, selection bias and long-tail phenomenon problem
(e.g., where very few sources provide the first information after a disaster) are challenging tasks for quasi real-time truth discovery.
\vspace*{-0.23cm}

\paragraph*{Estimation of Incompleteness, Biases and Errors in the Truth Discovery Process}  
Information without context can be easily distorted and misinterpreted.
When a piece of information is extracted from its original 
content, channel or thread, it may lose its context along with important 
\textquote{semantic markers} that explain \emph{when, where, how, why,} and
\emph{for which} purpose or audience it has been produced. Observation may also be incomplete and biased for various 
reasons, e.g., security and privacy concerns, format limitations, \emph{observer's bias} or \emph{disclosure bias}. 
Estimating the biases and errors along the entire truth discovery pipeline is crucial and challenging.
%

To overcome these challenges, we believe that an integrative framework is needed:
\begin{inparaenum}[(i)]
\item To define, in a principled way, a unified semantics of truth discovery;
\item To pro-actively collect new evidences, contextual data, and external knowledge from multi-modal data; 
\item To support continuous inference and belief revision for computing and updating data veracity estimates;
\item And finally, to monitor and estimate errors and biases in the truth discovery process.
\end{inparaenum}


To address these challenges, we have proposed DAFNA (\emph{Data Forensics with Analytics} -- {\url{dafna.qcri.org}}) 
at QCRI, an ambitious project for determining the veracity of  cross-modal information from multiple Web sources. Beyond 
a first module demonstrated in \cite{Wagui15}, DAFNA's vision is to provide a platform for actionable and cross-modal truth
discovery.
 
 
 
% Bibliography 

\bibliographystyle{abbrv}
\bibliography{references2}
                             % Sample .bib file with references that match those in
                             % the 'Specifications Document (V1.5)' as well containing
                             % 'legacy' bibs and bibs with 'alternate codings'.
                             % Gerry Murray - March 2012

% History dates
%\received{November 2015}{**** ****}{**** ****}
\end{document}
% End of v2-acmsmall-sample.tex (March 2012) - Gerry Murray, ACM


