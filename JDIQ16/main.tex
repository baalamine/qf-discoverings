% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
%\usepackage{mathptm}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[ruled]{algorithm2e}
\usepackage{csquotes}
\usepackage{paralist}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}
% Metadata Information
\acmVolume{6}
\acmNumber{6}
\acmArticle{6}
\acmYear{2015}
\acmMonth{11}

% Document starts
\begin{document}

% HEADERS
\markboth{L. Berti-Equille and M. L. BA}{Actionable Truth Discovery Challenges}
% TITLE
\title{Actionable Truth Discovery Challenges}
% AUTHORS AND AFFILIATION
\author{LAURE BERTI-EQUILLE, MOUHAMADOU LAMINE BA
\affil{Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar}
}

\category{H.3.3}{Information Storage and Retrieval}{Information Search and Retrieval}
\category{H.2.0}{Database Management}{General}
\terms{Algorithms, Management, Verification}
\keywords{Truth Discovery, Fact-checking, Data Quality, Data Fusion, Information Extraction}
\acmformat{Laure Berti-Equille, Mouhamadou Lamine Ba,  2015. Actionable Truth Discovery Challenges.}


\begin{bottomstuff}
Author's address: L. Berti-Equille, M. L. Ba, Qatar Computing Research Institute(Current address), Hamad
Bin Khalifa University, Tornado Tower 18th, P.O. Box 5825, Doha, Qatar.
\end{bottomstuff}

\maketitle


\section{Introduction}
As online user-generated content grows exponentially,
the reliance on Web data and information from social 
networking is increasing in many domains for various private and corporate usages. 
One of the fundamental difficulties is that data can be biased, noisy, outdated, incorrect, misleading and
thus unreliable. Massive data from multiple sources amplifies this problem since conflicting information
have to be \textquote{aligned}, compared  and check to estimate their veracity. Obviously, truth discovery from 
the Web has significant practical importance: online rumor propagation~\cite{rumor-icdm2013}, mis- or disinformation
can have tremendous impacts on our society, economy, politics, and homeland security. Online fact-checkers (e.g., FactCheck -- \url{http://www.factcheck.org/}) 
and humanitarian initiatives~\cite{ImranECDM13} have rapidly appeared as unavoidable for 
classifying and verifying (semi-automatically) online information. Beyond social media and computational journalism, 
the truth discovery problem has been also largely studied in both the artificial intelligence and the database communities, sometimes under the 
names of  \textquote{fact-checking}~\cite{GoasdoueKKLMZ13}, \textquote{information trustworthiness}~\cite{Thirunarayan2014182},
\textquote{information credibility}~\cite{PasternackR13}, or \textquote{information corroboration}~\cite{GallandAMS10}. 



The ultimate goal of truth discovery is to predict the truth label of a set of assertions from multiple sources
 and to infer sources' reliability with no \emph{a priori} knowledge about the truthfulness of the assertions and the sources. One 
major line of previous work, relaxing such a assumption, extended fact-finding models with prior knowledge either about the assertions~\cite{PasternackR13} 
or about the source reputation via trust assessment~\cite{BalakrishnanK11}. Another line of research aimed at iteratively computing and updating the source's
trustworthiness as a belief function in its claims, and then the belief score of each claim as a function of its sources' trustworthiness~\cite{YinHY08}, with 
probabilistic models which incorporate domain-specific aspects like source dependence~\cite{DongBHS10a}, evolving truth~\cite{DongBS09a}, hardness of certain 
claims~\cite{GallandAMS10}, complex data structures~\cite{ZhaoRGH12,GoasdoueKKLMZ13}, or multiple source's expertise~\cite{Ma15}. However, current models suffer 
from various limitations, e.g., strong assumptions about claims and sources, restricting their usability to the wide diversity of Web information and realistic 
scenarios where knowing the truth as soon as possible is vital. 
 We argue that the \emph{next-generation data-sharing systems} need to manage not only heterogeneity but also unreliable 
 information from various channels, in different languages, formats, and at different paces. Truth discovery systems 
 have to be set up to help institutions and citizens by providing rigorous, scientific explanations and reports 
 of their findings and collected evidences. Although some initial effort towards such a vision,
 e.g., \cite{DongS2013,DongBHS10a,LiDLMS12}, a more fundamental paradigm shift in data
 management to solve the truth discovery problem natively is in its infancy. Formally, this goes beyond adding layers and extensions to
 data fusion heuristics, data provenance or truth discovery models. Technically, the challenges are not only to  design 
 techniques and prototypes of truth discovery systems but also to democratize operational tools for Web scale \emph{information
 triage} and veracity verification.
  
  
  

 
%\vspace{-0.1cm}
% ****************** ARCHITECTURE ****************************************
\section{Actionable Truth Discovery Challenges}
Apart limitations of actual truth discovery systems such as constraining assumptions, complex model settings, and scalability issues, other
prominent challenges must be \emph{actioned} to effectively tackle the truth discovery problem in real-world cases. We highlight appealing 
ones and briefly present our vision of a solution.

\paragraph*{Timely and Actionable Truth Discovery} 
Accessing past events and historical data in truth discovery is surely useful, however,
from a humanitarian perspective, telling the truth from quasi real-time data 
could save lives. As a consequence, information extraction and triage as well as actionable
truth discovery computation need to be streamlined, prioritized,  and adjusted to the level 
of emergency and incompleteness of available information regarding the communities that will 
use it. This important usage-driven aspect goes beyond consistency checking and constraints.
%

\paragraph*{Data Alignment Across Languages, Formats, and Channels} 
Agility of a truth discovery system is of utmost importance both at 
the technical, structural, and semantic levels. 
Agility is the ability of the system 
to efficiently extract and map information: 
\begin{inparaenum}[(i)]
\item from various languages;
\item various formats and data structures; and
\item supported by various media and technologies.
\end{inparaenum}
For instance, actual information extraction applications (e.g., Open-Calais -- \url{http://www.opencalais.com})  
are often specific to a single channel/application/language and cannot be considered as \textquote{agile}. Moreover,
each step of textual content analysis produces systematic and random errors that 
require to be considered in truth discovery unlike previous attempts like~\cite{GoasdoueKKLMZ13}). 
Tracing and estimating  the errors of data extraction, formatting, and linking
is one of the main challenges for automating truth discovery. 
%

\paragraph*{Incomplete and Biased Observations}  
Observation data may be incomplete and biased for various 
reasons, e.g., security and privacy concerns, format limitations, and 
(unspecified) inferred values, leading to a truth discovery context where Open World
Assumption must hold. Modeling the data provider's/observer's is crucial since online 
sources have  various levels of {\it a priori} knowledge about the observation data 
collection technique, opportunistic data are {\it biased by  observation effort}, and the underlying
observation method is unknown. In addition, information may also suffer from \emph{observer's bias} and
\emph{disclosure bias}. One challenge is thus to estimate the incompleteness and biases and take them into
account in the truth discovery computation.

\paragraph*{Decontextualization and Distortion}  
Information without context can be easily distorted and misinterpreted.
Indeed, when a piece of information is extracted from its original 
content and channel, it may lose its context along with important 
\textquote{semantic markers} that explain \emph{when, where, how, why,} and
\emph{for which purpose/audience} it has been produced.
Moreover, the pace has to be taken into account when information updates from 
various sources have to be \emph{aligned} with considering the source granularity 
(e.g., an individual or an organization). 
Despites much research effort~\cite{eps270829} in modeling contexts, current information
extraction techniques constraint the 
context representation and can usually not handle a wide, unpredictable range of topics. Ultimately,
all meta-information about the context and sources' 
characteristics have to be encoded as evidences and analyzed for truth discovery computation.

\iffalse
Besides challenges given above, \emph{cross-modal} and \emph{dynamic} truth discovery
are also in their infancy. Cross-modality is needed because of the multiple possible facets,
e.g., textual, pictures, and movies, of the same information that need to be reconcilied.
Dynamicity is due by the fact recieved information could be subjet to multiple transformations
through the transmission media or intermediate applications during its life cycle. We believe that directions
for tacking these appealing truth discovery problems could be 
\fi

An interesting research direction to tackle these appealing truth discovery challenges could be an integrative
framework. We believe that such a framework has to
\begin{inparaenum}[(i)]
\item to define, in a principled way, a concrete and unified semantics of truth discovery systems, e.g., using
modal logics via axioms and canonical 
\emph{Kripke} structures~\cite{GorankoOtto06};
\item to support continuous inferrence and belief revision from new evidences, contextual data, and knowledge, inspired by recent development
in continuous cleaning~\cite{VCSM14};
%new claims, contextual metadata, prior beliefs, and background knowledge in the computation; and
\item and finally to monitor and report uncertainty generated by the truth discovery process itself during information processing.
\end{inparaenum}




% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{references}
                             % Sample .bib file with references that match those in
                             % the 'Specifications Document (V1.5)' as well containing
                             % 'legacy' bibs and bibs with 'alternate codings'.
                             % Gerry Murray - March 2012

% History dates
\received{November 2015}{**** ****}{**** ****}


\medskip

\end{document}
% End of v2-acmsmall-sample.tex (March 2012) - Gerry Murray, ACM


